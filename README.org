Why does mclapply take so much memory?

Most useRs know about the parallel::mclapply function which can
potentially speed up R code (with respect to the standard lapply
function) by executing in parallel across several cores. However I did
not know that this speedup comes at the price of increased memory
usage. In this report I will explore the memory usage of mclapply.

In [[file:figure-kilobytes-used.R]] I benchmarked the memory usage of
=LAPPLY(1:N, returnXXX)= for several different choices of LAPPLY
(lapply, mclapply), N (10, ..., 100000), returnXXX. The figure below
shows the memory usage as a function of number of iterations (N) for
two such returnXXX functions:

*returnNULL* is a function that just returns NULL. In this case
mclapply has a surprisingly significant linear memory overhead. For
example, about 60 megabytes of memory are required on my system for
mclapply to process a vector of N=100000 indices, returning a list of
100000 NULL (orange dots). In comparison, the standard lapply function
has no such overhead (constant memory usage, blue dots).

[[file:figure-kilobytes-used.png]]

*returnDF* is a function that just returns a pre-computed data.frame
with 100 rows. In this case the memory overhead is also linear, but
the linear factor is much bigger (about 800 megabytes of memory
required to process N=100000 indices). Again the standard lapply
function has no such overhead.

Is there any way to use mclapply without this memory overhead?

*** Reproducing these results

Copy works_with_R from
https://github.com/tdhock/dotfiles/blob/master/.Rprofile to your
~/.Rprofile, then on the command line cd to this directory and type
make.

*** Comparison with multiprocessing in Python

I adapted David Taylor's [[file:multiprocess.py]] code and ran it with
several parameters using [[file:multiprocess.sh]]. The figure below shows
an analogous benchmark for the multiprocessing module in Python:

[[file:figure-multiprocess.png]]

It seems that Python also suffers from the linear memory overhead.
